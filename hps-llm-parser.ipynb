{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3596287",
   "metadata": {},
   "source": [
    "# Using LLMs to parse History of Philosophy of Science (HPS) books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40208919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla V100-SXM2-32GB\n",
      "Memory Usage:\n",
      "Allocated: 18.0 GB\n",
      "Cached:    18.3 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bd5a9",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b761754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data/gleick2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a736e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for fname in DATA_DIR.glob(\"*\"):\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        md_content = f.read()\n",
    "        html = markdown.markdown(md_content)\n",
    "        plain_text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "        out[fname.stem] = plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9826aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Information\n",
      "horizon. Whether Ong would have seen cyberspace as fundamentally oral or literary, he would surely have recognized it as transformative: not just a revitalization of older forms, not just an amplification, but something wholly new. He might have sensed a coming discontinuity akin to the emergence of literacy itself. Few understood better than Ong just how profound a discontinuity that had been.\n",
      "When he began his studies, “oral literature” was a common phrase. It is an oxymoron laced with anachronism; the words imply an all-too-unconscious approach to the past by way of the present. Oral literature was generally treated as a variant of writing; this, Ong said, was “rather like thinking of horses as automobiles without wheels.”\n",
      "You can, of course, undertake to do this. Imagine writing a treatise on horses (for people who have never seen a horse) which starts with the concept not of “horse” but of “automobile,” built on the readers’ direct experience of automobiles. It proceeds to discourse on horses by always referring to them as “wheelless automobiles,” explaining to highly automobilized readers all the points of difference…. Instead of wheels, the wheelless automobiles have enlarged toenails called hooves; instead of headlights, eyes; instead of a coat of lacquer, something called hair; instead of gasoline for fuel, hay, and so on. In the end, horses are only what they are not.\n",
      "When it comes to understanding the preliterate past, we modern folk are hopelessly automobilized. The written word is the mechanism by which we know what we know. It organizes our thought. We may wish to understand the rise of literacy both historically and logically, but history and logic are themselves the products of literate thought.\n",
      "Writing, as a technology, requires premeditation and special art. Language is not a technology, no matter how well developed and efficacious. It is not best seen as something separate from the mind; it is what the mind does. “Language in fact bears the same relationship to the concept of mind that legislation bears to the concept of parliament,” says Jonathan Miller: “it is a competence forever bodying itself in a series of concrete performances.” Much the same might be said of writing—it is concrete performance—but when the word is instantiated in paper or stone, it takes on a separate existence as artifice. It is a product of tools, and it is a tool. And like many technologies that followed, it thereby inspired immediate detractors.\n",
      "One unlikely Luddite was also one of the first long-term beneficiaries. Plato (channeling the nonwriter Socrates) warned that this technology meant impoverishment:\n",
      "For this invention will produce forgetfulness in the minds of those who learn to use it, because they will not practice their memory. Their trust in writing, produced by external characters which are no part of themselves, will discourage the use of their own memory within them. You have invented an elixir not of memory, but of reminding; and you offer your pupils the appearance of wisdom, not true wisdom.\n"
     ]
    }
   ],
   "source": [
    "print(out[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788400f0",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0534f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"/gpfs1/llm/llama-3.2-hf/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "classifier = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4fb07",
   "metadata": {},
   "source": [
    "### Parse text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f710807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'Extract people information of the following text as JSON format. I want name, time, place, and role, if available. Do not justify you answers, put the well formated JSON inside a ```json ``` delimiter.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29b78595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Extract people information of the following text as JSON format. I want name, time, place, and role, if available. Do not justify you answers, put the well formated JSON inside a ```json ``` delimiter.\n",
      "\n",
      "### Input:\n",
      "The Information\n",
      "horizon. Whether Ong would have seen cyberspace as fundamentally oral or literary, he would surely have recognized it as transformative: not just a revitalization of older forms, not just an amplification, but something wholly new. He might have sensed a coming discontinuity akin to the emergence of literacy itself. Few understood better than Ong just how profound a discontinuity that had been.\n",
      "When he began his studies, “oral literature” was a common phrase. It is an oxymoron laced with anachronism; the words imply an all-too-unconscious approach to the past by way of the present. Oral literature was generally treated as a variant of writing; this, Ong said, was “rather like thinking of horses as automobiles without wheels.”\n",
      "You can, of course, undertake to do this. Imagine writing a treatise on horses (for people who have never seen a horse) which starts with the concept not of “horse” but of “automobile,” built on the readers’ direct experience of automobiles. It proceeds to discourse on horses by always referring to them as “wheelless automobiles,” explaining to highly automobilized readers all the points of difference…. Instead of wheels, the wheelless automobiles have enlarged toenails called hooves; instead of headlights, eyes; instead of a coat of lacquer, something called hair; instead of gasoline for fuel, hay, and so on. In the end, horses are only what they are not.\n",
      "When it comes to understanding the preliterate past, we modern folk are hopelessly automobilized. The written word is the mechanism by which we know what we know. It organizes our thought. We may wish to understand the rise of literacy both historically and logically, but history and logic are themselves the products of literate thought.\n",
      "Writing, as a technology, requires premeditation and special art. Language is not a technology, no matter how well developed and efficacious. It is not best seen as something separate from the mind; it is what the mind does. “Language in fact bears the same relationship to the concept of mind that legislation bears to the concept of parliament,” says Jonathan Miller: “it is a competence forever bodying itself in a series of concrete performances.” Much the same might be said of writing—it is concrete performance—but when the word is instantiated in paper or stone, it takes on a separate existence as artifice. It is a product of tools, and it is a tool. And like many technologies that followed, it thereby inspired immediate detractors.\n",
      "One unlikely Luddite was also one of the first long-term beneficiaries. Plato (channeling the nonwriter Socrates) warned that this technology meant impoverishment:\n",
      "For this invention will produce forgetfulness in the minds of those who learn to use it, because they will not practice their memory. Their trust in writing, produced by external characters which are no part of themselves, will discourage the use of their own memory within them. You have invented an elixir not of memory, but of reminding; and you offer your pupils the appearance of wisdom, not true wisdom.\n",
      "### Response:\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Ong\",\n",
      "    \"time\": \"when he began his studies\",\n",
      "    \"place\": \"not specified\",\n",
      "    \"role\": \"scholar\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Jonathan Miller\",\n",
      "    \"time\": \"not specified\",\n",
      "    \"place\": \"not specified\",\n",
      "    \"role\": \"philosopher\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Plato\",\n",
      "    \"time\": \"not specified\",\n",
      "    \"place\": \"not specified\",\n",
      "    \"role\": \"philosopher\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Socrates\",\n",
      "    \"time\": \"not specified\",\n",
      "    \"place\": \"not specified\",\n",
      "    \"role\": \"philosopher\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{out['20']}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "result = classifier(prompt, max_new_tokens=400, temperature=0.0, do_sample=False)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3af17ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(text):\n",
    "    start = text.find('```\\n') + len('```\\n')\n",
    "    end = text.find('\\n```', start)\n",
    "    json_str = text[start:end]\n",
    "    people = json.loads(json_str)\n",
    "    print(json.dumps(people, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64fb7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"Ong\",\n",
      "        \"time\": \"when he began his studies\",\n",
      "        \"place\": \"not specified\",\n",
      "        \"role\": \"scholar\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Jonathan Miller\",\n",
      "        \"time\": \"not specified\",\n",
      "        \"place\": \"not specified\",\n",
      "        \"role\": \"philosopher\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Plato\",\n",
      "        \"time\": \"not specified\",\n",
      "        \"place\": \"not specified\",\n",
      "        \"role\": \"philosopher\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Socrates\",\n",
      "        \"time\": \"not specified\",\n",
      "        \"place\": \"not specified\",\n",
      "        \"role\": \"philosopher\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "parse_json(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b56c43e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{out['21']}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "result = classifier(prompt, max_new_tokens=400, temperature=0.0, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1626ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93f8ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"Plato\",\n",
      "        \"time\": \"Two thousand years ago\",\n",
      "        \"place\": \"Western world\",\n",
      "        \"role\": \"made this observation\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"McLuhan\",\n",
      "        \"time\": \"End of the nineteenth century\",\n",
      "        \"place\": \"Western world\",\n",
      "        \"role\": \"said\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Pliny\",\n",
      "        \"time\": \"End of the nineteenth century\",\n",
      "        \"place\": \"Western world\",\n",
      "        \"role\": \"wrote\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Samuel Butler\",\n",
      "        \"time\": \"End of the nineteenth century\",\n",
      "        \"place\": \"Western world\",\n",
      "        \"role\": \"observed\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Theseus\",\n",
      "        \"time\": \"Ancient times\",\n",
      "        \"place\": \"Ancient Greece\",\n",
      "        \"role\": \"unwound Ariadne\\u2019s thread\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Ariadne\",\n",
      "        \"time\": \"Ancient times\",\n",
      "        \"place\": \"Ancient Greece\",\n",
      "        \"role\": \"the subject of Ariadne\\u2019s thread\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "parse_json(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30555d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_setup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
