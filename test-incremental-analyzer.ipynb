{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Incremental HPS Analyzer\n",
    "\n",
    "This notebook tests the incremental knowledge-building system that:\n",
    "- Remembers entities across chunks\n",
    "- Updates entities with new information\n",
    "- Tracks relationships and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs1/home/j/s/jstonge1/hps-llm-parser/incremental_hps_analyzer.py:16: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \\   Later: \"Ong said\"  /\n"
     ]
    }
   ],
   "source": [
    "from incremental_hps_analyzer import IncrementalHPSAnalyzer, KnowledgeBase\n",
    "import outlines\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6272e92b35044bebba8c0acec22ecbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load your model\n",
    "model_path = \"/gpfs1/llm/llama-3.2-hf/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model = outlines.models.transformers(\n",
    "    model_path,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental HPS Analyzer initialized!\n",
      "Starting with 0 known entities\n"
     ]
    }
   ],
   "source": [
    "# Initialize the incremental analyzer\n",
    "analyzer = IncrementalHPSAnalyzer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    token_max=900,\n",
    "    chunk_size=400  # Smaller chunks for better incremental tracking\n",
    ")\n",
    "\n",
    "print(\"Incremental HPS Analyzer initialized!\")\n",
    "print(f\"Starting with {len(analyzer.knowledge_base.entities)} known entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19.md: 2275 characters\n",
      "Loaded 20.md: 3132 characters\n",
      "Loaded 21.md: 3152 characters\n",
      "Loaded 22.md: 2962 characters\n",
      "Loaded 23.md: 3042 characters\n",
      "Loaded 24.md: 3135 characters\n",
      "Loaded 25.md: 3097 characters\n",
      "Loaded 26.md: 3854 characters\n",
      "\n",
      "Total text length: 24825 characters\n",
      "Estimated word count: 3868 words\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple pages to see incremental learning\n",
    "DATA_DIR = Path(\"./data/gleick2011/pages\")\n",
    "\n",
    "# Load several pages in order\n",
    "pages_to_analyze = [\"19.md\", \"20.md\", \"21.md\", \"22.md\", \"23.md\", \"24.md\", \"25.md\", \"26.md\"]  # Add more as needed\n",
    "combined_text = \"\"\n",
    "\n",
    "for page_file in pages_to_analyze:\n",
    "    if (DATA_DIR / page_file).exists():\n",
    "        with open(DATA_DIR / page_file, 'r', encoding='utf-8') as f:\n",
    "            page_content = f.read()\n",
    "            combined_text += f\"\\n\\n=== PAGE {page_file} ===\\n\\n\" + page_content\n",
    "            print(f\"Loaded {page_file}: {len(page_content)} characters\")\n",
    "\n",
    "print(f\"\\nTotal text length: {len(combined_text)} characters\")\n",
    "print(f\"Estimated word count: {len(combined_text.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING INCREMENTAL ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Starting Incremental HPS Analysis</span>                                                                               <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Text length: 24825 characters                                                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Starting knowledge: 0 entities                                                                                  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m \u001b[1;34mStarting Incremental HPS Analysis\u001b[0m                                                                               \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Text length: 24825 characters                                                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Starting knowledge: 0 entities                                                                                  \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Processing chunk_001 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mProcessing chunk_001 \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">New entities: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | <span style=\"color: #000080; text-decoration-color: #000080\">References to known: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0</span> | <span style=\"color: #af00ff; text-decoration-color: #af00ff\">New relationships: </span><span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32mNew entities: \u001b[0m\u001b[1;32m0\u001b[0m | \u001b[34mReferences to known: \u001b[0m\u001b[1;34m0\u001b[0m | \u001b[38;5;129mNew relationships: \u001b[0m\u001b[1;38;5;129m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Processing chunk_002 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mProcessing chunk_002 \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">New entities: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | <span style=\"color: #000080; text-decoration-color: #000080\">References to known: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0</span> | <span style=\"color: #af00ff; text-decoration-color: #af00ff\">New relationships: </span><span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">6</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32mNew entities: \u001b[0m\u001b[1;32m0\u001b[0m | \u001b[34mReferences to known: \u001b[0m\u001b[1;34m0\u001b[0m | \u001b[38;5;129mNew relationships: \u001b[0m\u001b[1;38;5;129m6\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Processing chunk_003 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mProcessing chunk_003 \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">New entities: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | <span style=\"color: #000080; text-decoration-color: #000080\">References to known: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32mNew entities: \u001b[0m\u001b[1;32m0\u001b[0m | \u001b[34mReferences to known: \u001b[0m\u001b[1;34m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Processing chunk_004 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mProcessing chunk_004 \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">New entities: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | <span style=\"color: #000080; text-decoration-color: #000080\">References to known: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32mNew entities: \u001b[0m\u001b[1;32m0\u001b[0m | \u001b[34mReferences to known: \u001b[0m\u001b[1;34m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Processing chunk_005 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mProcessing chunk_005 \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run incremental analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING INCREMENTAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# This will process chunks sequentially, building knowledge as it goes\n",
    "results = analyzer.analyze_text_incrementally(\n",
    "    text=combined_text,\n",
    "    format_output=True  # Shows progress as it builds knowledge\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted analysis of {len(results)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the knowledge base that was built\n",
    "kb = analyzer.knowledge_base\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KNOWLEDGE BASE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nTotal entities discovered: {len(kb.entities)}\")\n",
    "\n",
    "# Group by type\n",
    "people = [e for e in kb.entities.values() if e.entity_type == \"person\"]\n",
    "works = [e for e in kb.entities.values() if e.entity_type == \"work\"]\n",
    "topics = [e for e in kb.entities.values() if e.entity_type == \"topic\"]\n",
    "\n",
    "print(f\"\\n📚 People: {len(people)}\")\n",
    "for person in people[:5]:  # Show first 5\n",
    "    print(f\"  • {person.canonical_name}\")\n",
    "    print(f\"    Mentions: {len(person.mentions)}\")\n",
    "    print(f\"    Name variants: {list(person.name_variants)}\")\n",
    "    if person.attributes:\n",
    "        print(f\"    Key info: {dict(list(person.attributes.items())[:3])}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\n📖 Works: {len(works)}\")\n",
    "for work in works[:3]:\n",
    "    print(f\"  • {work.canonical_name}\")\n",
    "    print(f\"    Mentions: {len(work.mentions)}\")\n",
    "    if work.attributes:\n",
    "        print(f\"    Info: {dict(list(work.attributes.items())[:2])}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\n🧠 Topics: {len(topics)}\")\n",
    "for topic in topics[:3]:\n",
    "    print(f\"  • {topic.canonical_name}\")\n",
    "    print(f\"    Mentions: {len(topic.mentions)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate incremental knowledge building\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INCREMENTAL KNOWLEDGE EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find an entity that appears multiple times\n",
    "multi_mention_entities = [e for e in kb.entities.values() if len(e.mentions) > 1]\n",
    "\n",
    "if multi_mention_entities:\n",
    "    # Show the most mentioned entity\n",
    "    top_entity = max(multi_mention_entities, key=lambda e: len(e.mentions))\n",
    "    \n",
    "    print(f\"\\n🎯 FOCUS: {top_entity.canonical_name}\")\n",
    "    print(f\"Total mentions: {len(top_entity.mentions)}\")\n",
    "    print(f\"Name variants: {list(top_entity.name_variants)}\")\n",
    "    print(f\"Final attributes: {top_entity.attributes}\")\n",
    "    \n",
    "    print(\"\\nMention progression:\")\n",
    "    for i, mention in enumerate(top_entity.mentions, 1):\n",
    "        print(f\"  {i}. {mention.chunk_id}: '{mention.mention_text}'\")\n",
    "        print(f\"     Context: {mention.context[:100]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No entities found with multiple mentions in this sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the knowledge base for future use\n",
    "analyzer.save_knowledge_base(\"gleick_knowledge_base.json\")\n",
    "\n",
    "# You can also export to examine the structure\n",
    "kb_data = kb.export_knowledge_base()\n",
    "\n",
    "print(\"Knowledge base statistics:\")\n",
    "print(f\"  Total entities: {kb_data['total_entities']}\")\n",
    "print(f\"  Chunks processed: {kb_data['total_chunks_processed']}\")\n",
    "print(f\"  Average mentions per entity: {sum(e['mention_count'] for e in kb_data['entities'].values()) / len(kb_data['entities']):.1f}\")\n",
    "\n",
    "# Show the structure\n",
    "print(\"\\nSample entity structure:\")\n",
    "if kb_data['entities']:\n",
    "    sample_key = next(iter(kb_data['entities']))\n",
    "    sample_entity = kb_data['entities'][sample_key]\n",
    "    print(json.dumps(sample_entity, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test name matching (the key feature)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NAME MATCHING TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test how well it matches different name forms\n",
    "test_names = [\n",
    "    \"Walter J. Ong\",\n",
    "    \"Ong\", \n",
    "    \"Walter Ong\",\n",
    "    \"W. J. Ong\",\n",
    "    \"Jonathan Miller\",\n",
    "    \"Miller\",\n",
    "    \"Plato\",\n",
    "    \"Socrates\"\n",
    "]\n",
    "\n",
    "print(\"Testing name resolution:\")\n",
    "for test_name in test_names:\n",
    "    entity = kb.find_entity_by_name(test_name)\n",
    "    if entity:\n",
    "        print(f\"  '{test_name}' -> {entity.canonical_name} ({len(entity.mentions)} mentions)\")\n",
    "    else:\n",
    "        print(f\"  '{test_name}' -> NOT FOUND\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
